{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCF.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i7GGqxeWEPst",
        "XnF2Ec-WERsH",
        "ln_GIs7fERyv"
      ],
      "authorship_tag": "ABX9TyMl2s20ed0RuCu9aXzrDRDn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aSafarpoor/Recommender_Systems/blob/main/NCF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xWA3r-J5EGVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A pytorch GPU implementation of He et al. \"Neural Collaborative Filtering**\n",
        "\n",
        "based on https://github.com/hexiangnan/neural_collaborative_filtering"
      ],
      "metadata": {
        "id": "mjR8QG53EIV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=256 \n",
        "lr=0.001 \n",
        "factor_num=16"
      ],
      "metadata": {
        "id": "gYOCOuSuEYcc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLVzMQ3EIrdD",
        "outputId": "aa880682-9646-4a5b-c9a4-2d43edbea699"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/MSc/codes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ms4MRdfIwBh",
        "outputId": "608b49bd-5ece-4e9b-c85a-4f62e305de4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MSc/codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "JGPplZhlERQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir recommendation\n",
        "%cd recommendation\n",
        "%mkdir NCF-Data\n",
        "%cd NCF-Data\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p6ls7KDOAGY",
        "outputId": "5fdddc5b-9229-4901-a37e-f8fbbfd394cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘recommendation’: File exists\n",
            "/content/drive/MyDrive/MSc/codes/recommendation\n",
            "mkdir: cannot create directory ‘NCF-Data’: File exists\n",
            "/content/drive/MyDrive/MSc/codes/recommendation/NCF-Data\n",
            "/content/drive/MyDrive/MSc/codes/recommendation/NCF-Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.train.rating\n",
        "# !wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.rating\n",
        "# !wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.negative"
      ],
      "metadata": {
        "id": "vvDH4o5yPBn0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5hjb4XKNERQJ"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        # dataset name \n",
        "        self.dataset = 'ml-1m'\n",
        "\n",
        "        # model name \n",
        "        self.model = 'NeuMF-end'\n",
        "\n",
        "        # paths\n",
        "        self.main_path = '/recommendation/NCF-Data/'\n",
        "\n",
        "        self.train_rating = self.main_path + '{}.train.rating'.format(self.dataset)\n",
        "        self.test_rating = self.main_path + '{}.test.rating'.format(self.dataset)\n",
        "        self.test_negative = self.main_path + '{}.test.negative'.format(self.dataset)\n",
        "\n",
        "        self.model_path = './models/'\n",
        "        self.GMF_model_path = self.model_path + 'GMF.pth'\n",
        "        self.MLP_model_path = self.model_path + 'MLP.pth'\n",
        "        self.NeuMF_model_path = self.model_path + 'NeuMF.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data utils:"
      ],
      "metadata": {
        "id": "i7GGqxeWEPst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import scipy.sparse as sp\n",
        "import torch.utils.data as data\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "_K5nnQQbEwP9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "config.train_rating "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "J_YCNmxNRqZE",
        "outputId": "bd9922eb-d74d-4cfa-d40d-fe6f844eb4da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MSc/codes/recommendation/NCF-Data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/recommendation/NCF-Data/ml-1m.train.rating'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vJWxhjDBEC7j"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_all(test_num=100):\n",
        "    ''' We load all the three file here to save time in each epoch. '''\n",
        "    train_data = pd.read_csv(\n",
        "\t\t'/content/drive/MyDrive/MSc/codes'+config.train_rating, \n",
        "\t\tsep='\\t', header=None, names=['user', 'item'], \n",
        "\t\tusecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
        "    \n",
        "    user_num = train_data['user'].max() + 1\n",
        "    item_num = train_data['item'].max() + 1\n",
        "\n",
        "    train_data = train_data.values.tolist()\n",
        "\n",
        "    # load ratings as a dok matrix\n",
        "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
        "    for x in train_data:\n",
        "        train_mat[x[0], x[1]] = 1.0\n",
        "\n",
        "    test_data = []\n",
        "    with open('/content/drive/MyDrive/MSc/codes' + config.test_negative, 'r') as fd:\n",
        "        line = fd.readline()\n",
        "        while line != None and line != '':\n",
        "            arr = line.split('\\t')\n",
        "            u = eval(arr[0])[0]\n",
        "            test_data.append([u, eval(arr[0])[1]])\n",
        "            for i in arr[1:]:\n",
        "                test_data.append([u, int(i)])\n",
        "            line = fd.readline()\n",
        "    return train_data, test_data, user_num, item_num, train_mat\t\t"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NCFData(data.Dataset):\n",
        "\tdef __init__(self, features, \n",
        "\t\t\t\tnum_item, train_mat=None, num_ng=0, is_training=None):\n",
        "\t\tsuper(NCFData, self).__init__()\n",
        "\t\t\"\"\" Note that the labels are only useful when training, we thus \n",
        "\t\t\tadd them in the ng_sample() function.\n",
        "\t\t\"\"\"\n",
        "\t\tself.features_ps = features\n",
        "\t\tself.num_item = num_item\n",
        "\t\tself.train_mat = train_mat\n",
        "\t\tself.num_ng = num_ng\n",
        "\t\tself.is_training = is_training\n",
        "\t\tself.labels = [0 for _ in range(len(features))]\n",
        "\n",
        "\tdef ng_sample(self):\n",
        "\t\tassert self.is_training, 'no need to sampling when testing'\n",
        "\n",
        "\t\tself.features_ng = []\n",
        "\t\tfor x in self.features_ps:\n",
        "\t\t\tu = x[0]\n",
        "\t\t\tfor t in range(self.num_ng):\n",
        "\t\t\t\tj = np.random.randint(self.num_item)\n",
        "\t\t\t\twhile (u, j) in self.train_mat:\n",
        "\t\t\t\t\tj = np.random.randint(self.num_item)\n",
        "\t\t\t\tself.features_ng.append([u, j])\n",
        "\n",
        "\t\tlabels_ps = [1 for _ in range(len(self.features_ps))]\n",
        "\t\tlabels_ng = [0 for _ in range(len(self.features_ng))]\n",
        "\n",
        "\t\tself.features_fill = self.features_ps + self.features_ng\n",
        "\t\tself.labels_fill = labels_ps + labels_ng\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn (self.num_ng + 1) * len(self.labels)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tfeatures = self.features_fill if self.is_training \\\n",
        "\t\t\t\t\telse self.features_ps\n",
        "\t\tlabels = self.labels_fill if self.is_training \\\n",
        "\t\t\t\t\telse self.labels\n",
        "\n",
        "\t\tuser = features[idx][0]\n",
        "\t\titem = features[idx][1]\n",
        "\t\tlabel = labels[idx]\n",
        "\t\treturn user, item ,label"
      ],
      "metadata": {
        "id": "LluptRhWEs8f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate"
      ],
      "metadata": {
        "id": "XnF2Ec-WERsH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1ma3QT0QERsH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hit(gt_item, pred_items):\n",
        "\tif gt_item in pred_items:\n",
        "\t\treturn 1\n",
        "\treturn 0"
      ],
      "metadata": {
        "id": "vGAPohD6FhR9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg(gt_item, pred_items):\n",
        "\tif gt_item in pred_items:\n",
        "\t\tindex = pred_items.index(gt_item)\n",
        "\t\treturn np.reciprocal(np.log2(index+2))\n",
        "\treturn 0"
      ],
      "metadata": {
        "id": "hefpe2e0FfwV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(model, test_loader, top_k):\n",
        "\tHR, NDCG = [], []\n",
        "\n",
        "\tfor user, item, label in test_loader:\n",
        "\t\tuser = user.cuda()\n",
        "\t\titem = item.cuda()\n",
        "\n",
        "\t\tpredictions = model(user, item)\n",
        "\t\t_, indices = torch.topk(predictions, top_k)\n",
        "\t\trecommends = torch.take(\n",
        "\t\t\t\titem, indices).cpu().numpy().tolist()\n",
        "\n",
        "\t\tgt_item = item[0].item()\n",
        "\t\tHR.append(hit(gt_item, recommends))\n",
        "\t\tNDCG.append(ndcg(gt_item, recommends))\n",
        "\n",
        "\treturn np.mean(HR), np.mean(NDCG)"
      ],
      "metadata": {
        "id": "7YIUo7UlFeRO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model:"
      ],
      "metadata": {
        "id": "ln_GIs7fERyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F "
      ],
      "metadata": {
        "id": "xpv0yz40Frdk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mbgu4dRCERyv"
      },
      "outputs": [],
      "source": [
        "class NCF(nn.Module):\n",
        "\tdef __init__(self, user_num, item_num, factor_num, num_layers,\n",
        "\t\t\t\t\tdropout, model, GMF_model=None, MLP_model=None):\n",
        "\t\tsuper(NCF, self).__init__()\n",
        "\t\t\"\"\"\n",
        "\t\tuser_num: number of users;\n",
        "\t\titem_num: number of items;\n",
        "\t\tfactor_num: number of predictive factors;\n",
        "\t\tnum_layers: the number of layers in MLP model;\n",
        "\t\tdropout: dropout rate between fully connected layers;\n",
        "\t\tmodel: 'MLP', 'GMF', 'NeuMF-end', and 'NeuMF-pre';\n",
        "\t\tGMF_model: pre-trained GMF weights;\n",
        "\t\tMLP_model: pre-trained MLP weights.\n",
        "\t\t\"\"\"\t\t\n",
        "\t\tself.dropout = dropout\n",
        "\t\tself.model = model\n",
        "\t\tself.GMF_model = GMF_model\n",
        "\t\tself.MLP_model = MLP_model\n",
        "\n",
        "\t\tself.embed_user_GMF = nn.Embedding(user_num, factor_num)\n",
        "\t\tself.embed_item_GMF = nn.Embedding(item_num, factor_num)\n",
        "\t\tself.embed_user_MLP = nn.Embedding(\n",
        "\t\t\t\tuser_num, factor_num * (2 ** (num_layers - 1)))\n",
        "\t\tself.embed_item_MLP = nn.Embedding(\n",
        "\t\t\t\titem_num, factor_num * (2 ** (num_layers - 1)))\n",
        "\n",
        "\t\tMLP_modules = []\n",
        "\t\tfor i in range(num_layers):\n",
        "\t\t\tinput_size = factor_num * (2 ** (num_layers - i))\n",
        "\t\t\tMLP_modules.append(nn.Dropout(p=self.dropout))\n",
        "\t\t\tMLP_modules.append(nn.Linear(input_size, input_size//2))\n",
        "\t\t\tMLP_modules.append(nn.ReLU())\n",
        "\t\tself.MLP_layers = nn.Sequential(*MLP_modules)\n",
        "\n",
        "\t\tif self.model in ['MLP', 'GMF']:\n",
        "\t\t\tpredict_size = factor_num \n",
        "\t\telse:\n",
        "\t\t\tpredict_size = factor_num * 2\n",
        "\t\tself.predict_layer = nn.Linear(predict_size, 1)\n",
        "\n",
        "\t\tself._init_weight_()\n",
        "\n",
        "\tdef _init_weight_(self):\n",
        "\t\t\"\"\" We leave the weights initialization here. \"\"\"\n",
        "\t\tif not self.model == 'NeuMF-pre':\n",
        "\t\t\tnn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
        "\t\t\tnn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
        "\t\t\tnn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
        "\t\t\tnn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
        "\n",
        "\t\t\tfor m in self.MLP_layers:\n",
        "\t\t\t\tif isinstance(m, nn.Linear):\n",
        "\t\t\t\t\tnn.init.xavier_uniform_(m.weight)\n",
        "\t\t\tnn.init.kaiming_uniform_(self.predict_layer.weight, \n",
        "\t\t\t\t\t\t\t\t\ta=1, nonlinearity='sigmoid')\n",
        "\n",
        "\t\t\tfor m in self.modules():\n",
        "\t\t\t\tif isinstance(m, nn.Linear) and m.bias is not None:\n",
        "\t\t\t\t\tm.bias.data.zero_()\n",
        "\t\telse:\n",
        "\t\t\t# embedding layers\n",
        "\t\t\tself.embed_user_GMF.weight.data.copy_(\n",
        "\t\t\t\t\t\t\tself.GMF_model.embed_user_GMF.weight)\n",
        "\t\t\tself.embed_item_GMF.weight.data.copy_(\n",
        "\t\t\t\t\t\t\tself.GMF_model.embed_item_GMF.weight)\n",
        "\t\t\tself.embed_user_MLP.weight.data.copy_(\n",
        "\t\t\t\t\t\t\tself.MLP_model.embed_user_MLP.weight)\n",
        "\t\t\tself.embed_item_MLP.weight.data.copy_(\n",
        "\t\t\t\t\t\t\tself.MLP_model.embed_item_MLP.weight)\n",
        "\n",
        "\t\t\t# mlp layers\n",
        "\t\t\tfor (m1, m2) in zip(\n",
        "\t\t\t\tself.MLP_layers, self.MLP_model.MLP_layers):\n",
        "\t\t\t\tif isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear):\n",
        "\t\t\t\t\tm1.weight.data.copy_(m2.weight)\n",
        "\t\t\t\t\tm1.bias.data.copy_(m2.bias)\n",
        "\n",
        "\t\t\t# predict layers\n",
        "\t\t\tpredict_weight = torch.cat([\n",
        "\t\t\t\tself.GMF_model.predict_layer.weight, \n",
        "\t\t\t\tself.MLP_model.predict_layer.weight], dim=1)\n",
        "\t\t\tprecit_bias = self.GMF_model.predict_layer.bias + \\\n",
        "\t\t\t\t\t\tself.MLP_model.predict_layer.bias\n",
        "\n",
        "\t\t\tself.predict_layer.weight.data.copy_(0.5 * predict_weight)\n",
        "\t\t\tself.predict_layer.bias.data.copy_(0.5 * precit_bias)\n",
        "\n",
        "\tdef forward(self, user, item):\n",
        "\t\tif not self.model == 'MLP':\n",
        "\t\t\tembed_user_GMF = self.embed_user_GMF(user)\n",
        "\t\t\tembed_item_GMF = self.embed_item_GMF(item)\n",
        "\t\t\toutput_GMF = embed_user_GMF * embed_item_GMF\n",
        "\t\tif not self.model == 'GMF':\n",
        "\t\t\tembed_user_MLP = self.embed_user_MLP(user)\n",
        "\t\t\tembed_item_MLP = self.embed_item_MLP(item)\n",
        "\t\t\tinteraction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
        "\t\t\toutput_MLP = self.MLP_layers(interaction)\n",
        "\n",
        "\t\tif self.model == 'GMF':\n",
        "\t\t\tconcat = output_GMF\n",
        "\t\telif self.model == 'MLP':\n",
        "\t\t\tconcat = output_MLP\n",
        "\t\telse:\n",
        "\t\t\tconcat = torch.cat((output_GMF, output_MLP), -1)\n",
        "\n",
        "\t\tprediction = self.predict_layer(concat)\n",
        "\t\treturn prediction.view(-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main:"
      ],
      "metadata": {
        "id": "syH7woQyER6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJDoSTGnHBML",
        "outputId": "55037153-0077-4ed0-8e84-d23ee9e511f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.backends.cudnn as cudnn\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "'''\n",
        "import model\n",
        "import config\n",
        "import evaluate\n",
        "import data_utils\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rcDqF9S2Fz5E",
        "outputId": "d05206ab-6850-47b9-b0ad-9b74636674cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport model\\nimport config\\nimport evaluate\\nimport data_utils\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser={}\n",
        "parser[\"lr\"] = 0.001          #help=\"learning rate\"\n",
        "parser[\"dropout\"] = 0.0       #help=\"dropout rate\"\n",
        "parser[\"batch_size\"] = 256    #help=\"batch size for training\"\n",
        "parser[\"epochs\"] = 20         #help=\"training epoches\"\n",
        "parser[\"top_k\"] = 10          #help=\"compute metrics@top_k\"\n",
        "parser[\"factor_num\"] = 32     #help=\"predictive factors numbers in the model\"\n",
        "parser[\"num_layers\"] = 3      #help=\"number of layers in MLP model\"\n",
        "parser[\"num_ng\"] = 4          #help=\"sample negative items for training\"\n",
        "parser[\"test_num_ng\"] = 99    #help=\"sample part of negative items for testing\"\n",
        "parser[\"out\"] = True          #help=\"save model or   not\"\n",
        "parser[\"gpu\"] = \"0\"           #help=\"gpu card ID\""
      ],
      "metadata": {
        "id": "Xz6BBB-XGVzD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = parser[\"gpu\"]\n",
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "e5tIFUkXGiXU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "%cd ..\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3dTqSU4QGgc",
        "outputId": "a518840c-fd37-4754-ac8a-bc6fc9a634e7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MSc/codes/recommendation\n",
            "/content/drive/MyDrive/MSc/codes\n",
            "/content/drive/MyDrive/MSc/codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################## PREPARE DATASET ##########################\n",
        "train_data, test_data, user_num ,item_num, train_mat = load_all()\n",
        "\n",
        "# construct the train and test datasets\n",
        "train_dataset = NCFData(\n",
        "\t\ttrain_data, item_num, train_mat, parser[\"num_ng\"], True)\n",
        "test_dataset = NCFData(\n",
        "\t\ttest_data, item_num, train_mat, 0, False)\n",
        "train_loader = data.DataLoader(train_dataset,\n",
        "\t\tbatch_size=parser[\"batch_size\"], shuffle=True, num_workers=4)\n",
        "test_loader = data.DataLoader(test_dataset,\n",
        "\t\tbatch_size=parser[\"test_num_ng\"]+1, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l151tuOlGkK7",
        "outputId": "a46601b5-de7c-4ff9-c53d-c9550d24dfe2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################### CREATE MODEL #################################\n",
        "if config.model == 'NeuMF-pre':\n",
        "\tassert os.path.exists(config.GMF_model_path), 'lack of GMF model'\n",
        "\tassert os.path.exists(config.MLP_model_path), 'lack of MLP model'\n",
        "\tGMF_model = torch.load(config.GMF_model_path)\n",
        "\tMLP_model = torch.load(config.MLP_model_path)\n",
        "else:\n",
        "\tGMF_model = None\n",
        "\tMLP_model = None\n",
        "\n",
        "model = NCF(user_num, item_num, parser[\"factor_num\"], parser[\"num_layers\"], \n",
        "\t\t\t\t\t\tparser[\"dropout\"], config.model, GMF_model, MLP_model)\n",
        "model.cuda()\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "if config.model == 'NeuMF-pre':\n",
        "\toptimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "else:\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "P8wnQqwyGmRz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter() # for visualization"
      ],
      "metadata": {
        "id": "9wtbsIxBTtqb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "rp26ifQSlVXQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCDEhRipER6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bb1b93-ff2d-48d5-ff4d-37e8ea49fb4d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/19418 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 19418/19418 [02:45<00:00, 117.42it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The time elapse of epoch 000 is: 00: 03: 18\n",
            "HR: 0.637\tNDCG: 0.368\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19418/19418 [02:42<00:00, 119.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The time elapse of epoch 001 is: 00: 03: 12\n",
            "HR: 0.662\tNDCG: 0.392\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19418/19418 [02:43<00:00, 118.61it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The time elapse of epoch 002 is: 00: 03: 12\n",
            "HR: 0.683\tNDCG: 0.410\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19418/19418 [02:44<00:00, 117.80it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The time elapse of epoch 003 is: 00: 03: 13\n",
            "HR: 0.684\tNDCG: 0.414\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19418/19418 [02:43<00:00, 118.43it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The time elapse of epoch 004 is: 00: 03: 11\n",
            "HR: 0.691\tNDCG: 0.417\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19418/19418 [02:43<00:00, 118.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The time elapse of epoch 005 is: 00: 03: 12\n",
            "HR: 0.697\tNDCG: 0.421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19418/19418 [02:43<00:00, 119.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 006 is: 00: 03: 10\n",
            "HR: 0.697\tNDCG: 0.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19418/19418 [02:44<00:00, 117.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 007 is: 00: 03: 14\n",
            "HR: 0.694\tNDCG: 0.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19418/19418 [02:41<00:00, 120.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 008 is: 00: 03: 08\n",
            "HR: 0.693\tNDCG: 0.423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19418/19418 [02:41<00:00, 119.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 009 is: 00: 03: 10\n",
            "HR: 0.689\tNDCG: 0.419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 18210/19418 [02:31<00:09, 123.40it/s]"
          ]
        }
      ],
      "source": [
        "########################### TRAINING #####################################\n",
        "count, best_hr = 0, 0\n",
        "for epoch in range(parser[\"epochs\"]):\n",
        "\tmodel.train() # Enable dropout (if have).\n",
        "\tstart_time = time.time()\n",
        "\ttrain_loader.dataset.ng_sample()\n",
        "\n",
        "\tfor user, item, label in tqdm(train_loader):\n",
        "\t\ttry:\n",
        "\t\t\tuser = user.cuda()\n",
        "\t\t\titem = item.cuda()\n",
        "\t\t\tlabel = label.float().cuda()\n",
        "\n",
        "\t\t\tmodel.zero_grad()\n",
        "\t\t\tprediction = model(user, item)\n",
        "\t\t\tloss = loss_function(prediction, label)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\t# writer.add_scalar('data/loss', loss.item(), count)\n",
        "\t\t\tcount += 1\n",
        "\t\texcept:\n",
        "\t\t\tbreak\n",
        "\n",
        "\n",
        "\tmodel.eval()\n",
        "\tHR, NDCG = metrics(model, test_loader, parser[\"top_k\"])\n",
        "\n",
        "\telapsed_time = time.time() - start_time\n",
        "\tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n",
        "\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "\tif HR > best_hr:\n",
        "\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
        "\t\tif parser[\"out\"]:\n",
        "\t\t\tif not os.path.exists(config.model_path):\n",
        "\t\t\t\tos.mkdir(config.model_path)\n",
        "\t\t\ttorch.save(model, \n",
        "\t\t\t\t'{}{}.pth'.format(config.model_path, config.model))\n",
        "\n",
        "print(\"End. Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format(\n",
        "\t\t\t\t\t\t\t\t\tbest_epoch, best_hr, best_ndcg))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6KXOBODTldWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}